{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banking Project\n",
    "\n",
    "***\n",
    "\n",
    ">The bank wants to improve their services. For instance, the bank managers have only vague idea, who is a good client (whom to offer some additional services) and who is a bad client (whom to watch carefully to minimize the bank loses). Fortunately, the bank stores data about their clients, the accounts (transactions within several months), the loans already granted, the credit cards issued. The bank managers hope to improve their understanding of customers and seek specific actions to improve services. A mere application of a discovery tool will not be convincing for them.  \n",
    "\n",
    ">To test a data mining approach to help the bank managers, it was decided to address two problems, a descriptive and a predictive one. While the descriptive problem was left open, the predictive problem is the prediction of whether a loan will end successfuly.\n",
    "\n",
    "> _ - in Banking Case Description, ECAC Moodle Page_\n",
    "\n",
    "***\n",
    "\n",
    "[Kaggle Challenge Page](https://www.kaggle.com/)\n",
    "\n",
    "The steps performed are as follows:\n",
    "* Data Loading and Preparation\n",
    "* Descriptive Data Mining & Feature Engineering\n",
    "* Predictive Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "For this work we will use the common tools in a data scientist and engineer arsenal. All of them work together in a seamless fashion, as well as with the Jupyter Notebook (this enhanced interactive document).\n",
    "\n",
    "* **Numpy** is the fundamental package for scientific computing with Python\n",
    "* **Pandas** provides high-performance, easy-to-use data structures (_e.g._ data frames) and data analysis tools\n",
    "* **Matplotlib** implements plotting functionality\n",
    "* **Scikit Learn** aggregates advanced machine learning tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "A key initial step in every data mining work is to prepare the data. This reduces the occurence of future unexpected behaviors and gives a preliminary insight over the \"raw\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **transactions** records describe transactions on accounts, representing dynamic characteristics of the accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv('./data/banking - transaction.csv', \n",
    "                              sep=';',\n",
    "                              parse_dates=['date'],\n",
    "                              infer_datetime_format=True,\n",
    "                              dtype={'bank':np.str},\n",
    "                              index_col='trans_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k_symbol** name is not very represent representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactions_df = transactions_df.rename(columns={\n",
    "    'k_symbol': 'trans_char'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accounts** records contain static characteristics of the accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_df = pd.read_excel('./data/banking.xlsx', \n",
    "                            sheetname='account',\n",
    "                            parse_dates=['date'],\n",
    "                            infer_datetime_format=True,\n",
    "                            index_col='account_id'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **clients** records describe static characteristics of the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clients_df = pd.read_excel('./data/banking.xlsx',\n",
    "                           sheetname='client',\n",
    "                           index_col='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clients_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **birth_number** feature is not readable in this representation. We have, then, to parse it and transform it into two new columns: **birthday** and **gender**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clients_df['gender'] = clients_df.apply(lambda c: 'Male' if c['birth_number'] % 10000 < 5000 else 'Female', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def normalize_birth_number(client):\n",
    "    birth_number = int(client['birth_number'])\n",
    "    year = birth_number // 10000\n",
    "    month = (birth_number // 100) % 100\n",
    "    day = birth_number % 100\n",
    "    \n",
    "    month = month if month < 50 else month - 50\n",
    "    \n",
    "    return  \"{0:02d}{1:02d}{2:02d}\".format(year, month, day)\n",
    "\n",
    "\n",
    "clients_df['birth_number'] = clients_df.apply(normalize_birth_number, axis=1) # month - 50 on females\n",
    "clients_df['birthday'] = pd.to_datetime(clients_df['birth_number'], format='%y%m%d')\n",
    "clients_df['birthday'] = clients_df.apply(\n",
    "    lambda c: c['birthday'] if c['birthday'].date() <= date.today() else (c['birthday'] - pd.tseries.offsets.DateOffset(years=100)), \n",
    "    axis=1) # if infered year > 2015 the it is in the 19's\n",
    "clients_df = clients_df.drop('birth_number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clients_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dispositions** records relate a client with an account (being useful in join operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dispositions_df = pd.read_excel('./data/banking.xlsx',\n",
    "                                sheetname='disposition',\n",
    "                                index_col='disp_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispositions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **payment_orders** records, like **transaction** records, represent another dynamic characteristic of accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "payment_orders_df = pd.read_excel('./data/banking.xlsx',\n",
    "                                  sheetname='payment order',\n",
    "                                  index_col='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "payment_orders_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **loans** records describe information of a loan for an account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans_df = pd.read_excel('./data/banking.xlsx',\n",
    "                         sheetname='loan',\n",
    "                         parse_dates=['date'],\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col='loan_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **credit_cards** records describes static information of a credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_cards_df = pd.read_excel('./data/banking.xlsx',\n",
    "                                sheetname='credit card',\n",
    "                                parse_dates=['issued'],\n",
    "                                infer_datetime_format=True,\n",
    "                                index_col='card_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_cards_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **districts** records provide demographic information about a district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "districts_df = pd.read_excel('./data/banking.xlsx',\n",
    "                             sheetname='district',\n",
    "                             index_col='A1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column labels provided lack any useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "districts_df = districts_df.rename(columns={\n",
    "        'A2': 'district_name',\n",
    "        'A3': 'region',\n",
    "        'A4': 'no_inhabitants',\n",
    "        'A5': 'no_municipalities_w_inhabitants_<499',\n",
    "        'A6': 'no_municipalities_w_inhabitants_500-1999',\n",
    "        'A7': 'no_municipalities_w_inhabitants_2000-9999',\n",
    "        'A8': 'no_municipalities_w_inhabitants_>10000',\n",
    "        'A9': 'no_cities',\n",
    "        'A10': 'ratio_urban_inhabitants',\n",
    "        'A11': 'average_salary',\n",
    "        'A12': 'unemployment_rate_95',\n",
    "        'A13': 'unemployment_rate_96',\n",
    "        'A14': 'no_enterpreneurs_per_1000_inhabitants',\n",
    "        'A15': 'no_commited_crimes_95',\n",
    "        'A16': 'no_commited_crimes_96',\n",
    "    })\n",
    "\n",
    "districts_df.index.name = 'district_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if the types infered by Pandas library are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that **unemployment_rate_95** and **no_commited_crimes_95** are loaded as objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df['unemployment_rate_95'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df['no_commited_crimes_95'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both use a question mark to demark missing values. We'll convert properly those columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df['unemployment_rate_95'] = pd.to_numeric(districts_df['unemployment_rate_95'], errors='coerce')\n",
    "districts_df['no_commited_crimes_95'] = pd.to_numeric(districts_df['no_commited_crimes_95'], errors='coerce')\n",
    "\n",
    "districts_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive Data Mining & Feature Engineering\n",
    "\n",
    "This first section aims at providing ways to better understand and extract value from the data. This is mostly accoplished by gathering descriptive statistics and ploting.\n",
    "\n",
    "Considering this gathered knowledge, the datasets are edited and joined into useful intermediate format, which represent the main entities in the data, and then in a format in which the machine learning algorithms are able to understand (most of the times a single matrix, and most of the times without missing values).\n",
    "\n",
    "The **loans** relate to the remainder entities through the **account** they are linked to. Therefore, the remainder entities should be summarized in such a way that each of the **accounts** information is given in a single row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before making any assumption, we should extract simple statistics about the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**type**, **operation** and **trans_char** seem all to represent the same information. Lets evaluate that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"type:\", transactions_df['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"operation:\", transactions_df['operation'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"trans_char:\", transactions_df['trans_char'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**operation** seems irrelevant give **type**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df_e = transactions_df.drop('operation', axis=1).copy() # 'e' for edited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its also irrelevant the distinction between *withrawal* and *withrawal in cash*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = transactions_df_e['type'] == 'withdrawal in cash'\n",
    "transactions_df_e.ix[mask, 'type'] = ('withdrawal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an additional column to store the signed amount (given by the type of operation), and another with the normalized **signed_amount** value, according to the **balance** previous to the operation (if an operation is the first one, we store 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df_e['signed_amount'] = transactions_df_e.apply(lambda x: - x['amount'] if x['type'] == 'withdrawal' else x['amount'], axis=1)\n",
    "transactions_df_e['norm_signed_amount'] = transactions_df_e.apply(lambda x: \n",
    "                                                                      0 if (x['balance'] - x['signed_amount']) == 0 \n",
    "                                                                      else x['signed_amount'] / (x['balance'] - x['signed_amount']), \n",
    "                                                                  axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **trans_char** we are able to extract if the user is pensionist, or if the user has been sanctioned for negative balance, among other things. We will create an additional table with that information, with the values weighted by the **amount**, indexed by **account_id**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select the needed rows from transactions_df\n",
    "trans_temp_df = transactions_df_e[['account_id', 'trans_char', 'norm_signed_amount']].copy()\n",
    "\n",
    "# remove the rows where an empty string is present\n",
    "mask = trans_temp_df.trans_char != ' '\n",
    "trans_temp_df = trans_temp_df.ix[mask]\n",
    "\n",
    "# remove the rows containing NaN\n",
    "trans_temp_df = trans_temp_df.dropna(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the dataframe indexed by account_id\n",
    "account_features_df = trans_temp_df[['account_id']].drop_duplicates(subset=['account_id'])\n",
    "account_features_df = account_features_df.set_index('account_id')\n",
    "\n",
    "# create the count columns, corresponding to the data countained in\n",
    "    # pension\n",
    "    # interest credited\n",
    "    # household\n",
    "    # statement\n",
    "    # insurance payment\n",
    "    # sanction for negative balance\n",
    "    # loan payment\n",
    "\n",
    "def create_trans_count_col(df, val):\n",
    "    new_df = df.ix[df['trans_char'] == val].groupby('account_id').sum()\n",
    "    new_df = new_df.rename(columns={'norm_signed_amount':val})\n",
    "    return new_df\n",
    "\n",
    "additional_dfs = [create_trans_count_col(trans_temp_df, val) for val in trans_temp_df['trans_char'].unique()]\n",
    "account_features_df = account_features_df.join(additional_dfs)\n",
    "\n",
    "account_features_df = account_features_df.fillna(0)\n",
    "\n",
    "account_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge it with the **accounts** dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accounts_df = accounts_df.join(account_features_df)\n",
    "\n",
    "# set to 0 the columns for accounts that do not have any transaction\n",
    "accounts_df = accounts_df.fillna(0)\n",
    "\n",
    "accounts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we'll look into the **normalized signed amount** and **type** of each operation.\n",
    "\n",
    "We will extract, for each **account**, the *count*, *mean* and *standard deviation* of operation values, as well as *mean* and *standard deviation* of the number of days between each **operation**, grouped by operation **type**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_df = transactions_df_e[['account_id', 'type', 'norm_signed_amount', 'date']].copy()\n",
    "\n",
    "# create columns with dates converted to days since 01-01-1970\n",
    "#temp_df['date_days'] = temp_df.apply(lambda x: (x['date'] - pd.datetime(1970,1,1)).days, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort firstly by account_id, then by date\n",
    "temp_df = temp_df.sort_values(by=['account_id', 'date'])\n",
    "\n",
    "#obtain, by row, the previous date\n",
    "prev_dates = temp_df.groupby('account_id').apply(lambda x: x['date'].shift().fillna(x.iloc[0]['date'])).reset_index(level=0)\n",
    "delta = temp_df['date'] - prev_dates['date']\n",
    "delta = delta.astype(\"timedelta64[D]\")\n",
    "temp_df['date_delta'] = delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#temp_df = temp_df.drop('date', axis='columns')\n",
    "\n",
    "def summarize_transactions(df):\n",
    "    summaries_dfs = []\n",
    "    for tp in df['type'].unique():\n",
    "        tmp_df = temp_df[temp_df['type'] == tp].drop('type', axis=1)\n",
    "        \n",
    "        tmp_grp_df = tmp_df.groupby('account_id').agg([np.count_nonzero, np.average, np.std])\n",
    "        tmp_grp_df.fillna(0)\n",
    "        \n",
    "        ops_df = tmp_grp_df['norm_signed_amount']\n",
    "        ops_df = ops_df.rename(columns={'count_nonzero': tp + '_cnt',\n",
    "                                        'average': tp + '_avg',\n",
    "                                        'std': tp + '_std',\n",
    "                                       })\n",
    "        \n",
    "        dates_df = tmp_grp_df['date_delta']\n",
    "        dates_df = dates_df.drop('count_nonzero', axis='columns')\n",
    "        dates_df = dates_df.rename(columns={'average': tp + '_dates_avg',\n",
    "                                            'std': tp + '_dates_std',\n",
    "                                           })\n",
    "        joined_summary_df = ops_df.join(dates_df)\n",
    "        summaries_dfs.append(joined_summary_df)\n",
    "    \n",
    "    # now concatenate the summaries_dfs\n",
    "    summaries_df = pd.concat(summaries_dfs, axis='columns')\n",
    "    return summaries_df\n",
    "\n",
    "trans_summaries_df = summarize_transactions(temp_df)\n",
    "trans_summaries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge with **accounts** dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_df = accounts_df.join(trans_summaries_df)\n",
    "accounts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_df[accounts_df['withdrawal_std'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clients\n",
    "\n",
    "Now we'll take a look at the features that characterize each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clients_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for each user corresponds a district, we'll look into the corresponding table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider both the individual districts or the regions. \n",
    "In order to evaluate if the generalization for regions is reduces or not some interesting events, we'll use PCA to reduce the dimensionality of the table to two dimensions, therefore making it possible to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df_e = districts_df.drop(['district_name', 'region'], axis='columns')\n",
    "districts_df_e = districts_df_e.fillna(0)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "districts_reduced = pca.fit_transform(districts_df_e.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df_e['a'] = districts_reduced[:, 0]\n",
    "districts_df_e['b'] = districts_reduced[:, 1]\n",
    "\n",
    "# convert regions to integers, to use them as colors\n",
    "regions = districts_df.apply(lambda x: x['region'][1], axis='columns').astype(int)\n",
    "\n",
    "districts_df_e.plot(kind='scatter', x='a', y='b', c=regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we conclude that probably it is beneficial to work with districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clients_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clients_districts_df = pd.merge(clients_df, districts_df.drop(['region', 'district_name'], axis='columns'), left_on='district_id', right_index=True, how='left', sort=False)\n",
    "\n",
    "# we no longer need district_id column\n",
    "clients_districts_df = clients_districts_df.drop('district_id', axis='columns')\n",
    "\n",
    "clients_districts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll merge **Credit Cards** and **Clients** information with the accounts.\n",
    "\n",
    "As there may be multiple credit cards and clients associated with a given account, we have to devise a certain heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispositions_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispositions_df.type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll simply use the information about the owner of the account.\n",
    "\n",
    "We'll check the data if there is really only one owner per account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispositions_owners_df = dispositions_df[dispositions_df['type'] == 'OWNER'].drop('type', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "owners_per_account_df = dispositions_owners_df.groupby(['account_id']).count()\n",
    "\n",
    "owners_per_account_df[owners_per_account_df['client_id'] != 1].count(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge **clients** with **accounts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_disp_merge = accounts_df.merge(dispositions_owners_df, left_index=True, right_on='account_id', how='left', sort=False)\n",
    "accounts_disp_merge = accounts_disp_merge.set_index('account_id')\n",
    "\n",
    "\n",
    "accounts_clients_df = accounts_disp_merge.merge(clients_districts_df, left_on='client_id', right_index=True, how='left', sort=False)\n",
    "accounts_clients_df = accounts_clients_df.drop('client_id', axis='columns')\n",
    "\n",
    "accounts_clients_df = accounts_clients_df.fillna(0)\n",
    "\n",
    "accounts_clients_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loans\n",
    "\n",
    "We want to keep a summary of the history of loans related to a given account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(loans_df.status.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference:\n",
    " * **A-** contract finished, no problems\n",
    " * **B-** contract finished, loan not payed\n",
    " * **C-** running contract, ok so far\n",
    " * **D-** running contract, client in debt\n",
    " \n",
    "Lets  see how the loans distribute accross the four categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans_df.groupby('status').count()['account_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also a lot of contracts running, but most importantly there is a gret disparity between the two counts of the different loans results. We must be aware of this when training the algorithms. As we want to identify properly the loans that will have the **B** status, we have to be carefull when structuring the training and testing datasets.\n",
    "\n",
    "In order to summarize this data, we want to obtain the *count* of loans in each state, as well as the *average* and *standard deviation* of the amount and duration associated with each loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summarize_loans(df):\n",
    "    summaries_dfs = []\n",
    "    for tp in df['status'].unique():\n",
    "        tmp_df = df[df['status'] == tp].drop('status', axis=1)\n",
    "        \n",
    "        tmp_grp_df = tmp_df.groupby('account_id').agg([np.count_nonzero, np.average, np.std])\n",
    "        \n",
    "        amount_df = tmp_grp_df['amount']\n",
    "        amount_df = amount_df.rename(columns={'count_nonzero': tp + '_cnt',\n",
    "                                           'average': tp + '_amount_avg',\n",
    "                                           'std': tp + '_amount_std',\n",
    "                                          })\n",
    "        \n",
    "        duration_df = tmp_grp_df['duration']\n",
    "        duration_df = duration_df.drop('count_nonzero', axis='columns')\n",
    "        duration_df = duration_df.rename(columns={'average': tp + '_duration_avg',\n",
    "                                                  'std': tp + '_duration_std',\n",
    "                                                 })\n",
    "        joined_summary_df = amount_df.join(duration_df)\n",
    "        summaries_dfs.append(joined_summary_df)\n",
    "    \n",
    "    # now concatenate the summaries_dfs\n",
    "    summaries_df = pd.concat(summaries_dfs, axis='columns')\n",
    "    return summaries_df\n",
    "\n",
    "\n",
    "loans_summary_df = summarize_loans(loans_df)\n",
    "loans_summary_df = loans_summary_df.fillna(0)\n",
    "loans_summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyse if the history of each account is long enough to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans_hist_df = loans_summary_df[['A_cnt', 'B_cnt', 'C_cnt', 'D_cnt']].sum(axis='columns')\n",
    "loans_hist_df.plot(kind='hist', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are no accounts with more than one loan through time. Therefore, we will not use the loans history of each account. We'll simply save a dataframe with this information merged with accounts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_loans_df = accounts_clients_df.join(loans_summary_df)\n",
    "accounts_loans_df = accounts_loans_df.fillna(0)\n",
    "accounts_loans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_loans_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've gathered 55 features to characterize each account, gathering information about the accounts, users (and respective credit cards), transactions and history of loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictive Data Mining\n",
    "\n",
    "The above-mentioned accounts-indexed dataframe, combined with the specification of each loan (duration and amount), constitutes all the needed information to train our algorithms.\n",
    "\n",
    "\n",
    "Firstly we have to prepare a dataframe such that the machine-learning algorithms are able to undertand it. In order to accomplish that, we will merge this **accounts_loans_df** dataframe with the original **loans_df** dataframe. For new loans that arive, the process will be the same, therefore we want to encapsulate the process in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_final_df = accounts_clients_df.rename(columns={'date': 'date created'})\n",
    "accounts_final_df['account_id'] = accounts_final_df.index\n",
    "\n",
    "def join_loans_accounts(l_df):\n",
    "    dataset = pd.merge(l_df, accounts_final_df, left_on='account_id', right_index=True, how='left', sort=False)\n",
    "    return dataset\n",
    "\n",
    "dataset = join_loans_accounts(loans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we have to extract the useful rows: the ones with **status** *A* and *B*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_useful_rows(df):\n",
    "    mask = (dataset.status == 'A') | (dataset.status == 'B')\n",
    "    return dataset[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_useful = select_useful_rows(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dataset.shape)\n",
    "print(dataset_useful.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just have now to convert the timestamp columns to an integer value. We'll use Unix time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_date_cols(df):\n",
    "    new_df = df.copy()\n",
    "    for col in new_df.columns:\n",
    "        if new_df[col].dtype == 'datetime64[ns]':\n",
    "            new_df[col] = new_df[col].astype(np.int64) // 10**9\n",
    "    return new_df\n",
    "            \n",
    "\n",
    "dataset_valid = convert_date_cols(dataset_useful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and convert categorical data in new columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_mask = dataset_valid.columns[dataset_valid.columns != 'status']\n",
    "\n",
    "dataset_valid_num = pd.get_dummies(dataset_valid[features_mask])\n",
    "dataset_valid_num['status'] = dataset_valid['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = dataset_valid_num.columns[dataset_valid_num.columns != 'status']\n",
    "label = 'status'\n",
    "target_names = ['A', 'B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing classifiers\n",
    "\n",
    "We'll start off by using a **decision tree** as our first machine learning classifier. It has the benefits of working both with continuous and discrete data types, and provides a very natural way to interpret the results it gives.\n",
    "\n",
    "We'll use stratified k-folds cross-validation to prevent overfitting.\n",
    "\n",
    "The next function implements the procedure of, for a given model, it splits the data into train and test sets, and next performs stratified k-fold cross-validation to select an instance of the model that performs best for the corresponding validation set. It returns the score of the classifier in the testing set, as well as the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.base import clone as skl_clone\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def k_fold_model_select(data, features, label, raw_classifier, n_folds=10, weigh_samples_fn=None): \n",
    "    # weigh_samples_fn is explained below\n",
    "    \n",
    "    # split into training and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features].values, \n",
    "                                                        data[label].values,\n",
    "                                                        test_size=0.3,\n",
    "                                                        stratify=data[label],\n",
    "                                                        random_state=5)\n",
    "    \n",
    "    \n",
    "    # use stratified k-fold cross validation to select the model\n",
    "    skf = StratifiedKFold(y_train, n_folds=n_folds)\n",
    "\n",
    "    best_classifier = None\n",
    "    best_score = float('-inf')\n",
    "\n",
    "    for train_index, validation_index in skf:\n",
    "        classifier = skl_clone(raw_classifier)\n",
    "        classifier = classifier.fit(X_train[train_index], y_train[train_index])\n",
    "\n",
    "        if weigh_samples_fn != None:\n",
    "            y_pred = classifier.predict(X_train[validation_index])\n",
    "            sample_weight = weigh_samples_fn(y_train[validation_index], y_pred)\n",
    "        else:\n",
    "            sample_weight = None\n",
    "            \n",
    "        score = classifier.score(X_train[validation_index], y_train[validation_index],\n",
    "                                 sample_weight=sample_weight)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_classifier = classifier\n",
    "            best_score = score\n",
    "    \n",
    "    # compute the confusion matrix\n",
    "    y_pred = best_classifier.predict(X_test)\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # now compute the score for the test data of the best found classifier\n",
    "    if weigh_samples_fn != None:\n",
    "        sample_weight = weigh_samples_fn(y_test, y_pred)\n",
    "    else:\n",
    "        sample_weight = None\n",
    "    test_score = best_classifier.score(X_test, y_test, sample_weight=sample_weight)\n",
    "    \n",
    "    # and obtain the classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    \n",
    "    return (test_score, report, conf_mat, best_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(min_samples_split=20, random_state=0)\n",
    "dtc_score, dtc_rep, dtc_cm, dtc_clf = k_fold_model_select(dataset_valid_num, features, label, dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "import pydot_ng as pydot\n",
    "from IPython.display import Image\n",
    "\n",
    "def display_tree(dtc_classifier):\n",
    "    dot_data = StringIO()  \n",
    "    tree.export_graphviz(dtc_clf, out_file=dot_data,  \n",
    "                         feature_names=features_mask,\n",
    "                         class_names=target_names,\n",
    "                         filled=True,\n",
    "                         rounded=True,\n",
    "                         special_characters=True)\n",
    "    graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "    return Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_tree(dtc_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the tree indicates that the accounts with low withdrawal values (*withdrawal_avg*) (relative to the account's balance) and high number of deposits (*credit_cnt*) are mostly classified as reliable accounts for loans.\n",
    "\n",
    "On the other hand, accounts with higher withdrawal values (also relative to the account's balance), and higher variance in deposits periodicity (*credit_dates_std*) are mostly classified as non-reliable accounts for loans.\n",
    "\n",
    "Let's analyse more deeply these results. For that, we'll use the obtained score in predicting the test set and the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_confusion_matrix(cm):\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "    # in each class)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    return cm_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Score for best Decision Tree Classifier:\", dtc_score)\n",
    "print(\"Confusion matrix:\", dtc_cm, sep='\\n')\n",
    "print(\"Classification report:\", dtc_rep, sep='\\n')\n",
    "\n",
    "plot_confusion_matrix(normalize_confusion_matrix(dtc_cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things seem prety good: we have a good accuracy, and the plotted confusion matrix shows high percentage of results in the main diagonal!\n",
    "\n",
    "Nonetheless, there are two factors we have to take into account: \n",
    " * As stated before, we have very few examples with label B (unsuccessful loans), and those events, despite unusual, are very costly. Therefore we need to give them higher relevance.\n",
    " * Trustworthy users that are labeled as trustless ones will most likely swithch to a different bank, negating all the future accumulated revenue. As shown by the confusion matrix, one of the users suffers from exactly this fenomena, therefore we must pay attention to that fact.\n",
    "\n",
    "Because of this, we must be pragmatic about these results and perform some changes.\n",
    "\n",
    "In order to gain more insight, we'll continue by testing other classifiers, analysing their ROC curves. \n",
    "\n",
    "Where applicable, we'll also set the weights given to each label to be **balanced**: this way the classifiers take into account the frequency of each class and automatically adjust their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(min_samples_split=20, random_state=0, class_weight='balanced')\n",
    "dt_score, dt_rep, dt_cm, dt_clf = k_fold_model_select(dataset_valid_num, features, label, dt)\n",
    "\n",
    "print(\"Score:\", dt_score)\n",
    "print(\"Confusion matrix:\", dt_cm, sep='\\n')\n",
    "print(\"Classification report:\", dt_rep, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear Classifier (Logistic Regression)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr_score, lr_rep, lr_cm, lr_clf = k_fold_model_select(dataset_valid_num, features, label, lr)\n",
    "\n",
    "print(\"Score:\", lr_score)\n",
    "print(\"Confusion matrix:\", lr_cm, sep='\\n')\n",
    "print(\"Classification report:\", lr_rep, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nearest Neighbors Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(weights='distance')\n",
    "knn_score, knn_rep, knn_cm, knn_clf = k_fold_model_select(dataset_valid_num, features, label, knn)\n",
    "\n",
    "print(\"Score:\", knn_score)\n",
    "print(\"Confusion matrix:\", knn_cm, sep='\\n')\n",
    "print(\"Classification report:\", knn_rep, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (Gaussian Naive Bayes)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb_score, nb_cm, nb_clf = k_fold_model_select(dataset_valid_num, features, label, nb)\n",
    "\n",
    "print(\"Score:\", nb_score)\n",
    "print(\"Confusion matrix:\", nb_cm, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Neural Network (Multi-Layer Perceptron)\n",
    "from sknn.mlp import Classifier as MLPClassifier\n",
    "from sknn.mlp import Layer as MLPLayer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# use a pipeline that first normalizes the data\n",
    "\n",
    "nn = MLPClassifier(\n",
    "    layers=[\n",
    "        MLPLayer(\"Maxout\", units=30, pieces=2),\n",
    "        MLPLayer(\"Softmax\")],\n",
    "    learning_rate=0.000000001,\n",
    "    n_iter=25)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('min/max scaler', MinMaxScaler(feature_range=(0.0, 1.0))),\n",
    "        ('neural network', nn)])\n",
    "\n",
    "nn_score, nn_rep, nn_cm, nn_clf = k_fold_model_select(dataset_valid_num, features, label, nn)\n",
    "\n",
    "print(\"Score:\", nn_score)\n",
    "print(\"Confusion matrix:\", nn_cm, sep='\\n')\n",
    "print(\"Classification report:\", nn_rep, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test SVC with different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVC (linear kernel)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#svc_linear = SVC(kernel='linear')\n",
    "#svc_linear_score, svc_linear_rep, svc_linear_cm, svc_linear_clf = k_fold_model_select(dataset_valid_num, features, label, svc_linear,\n",
    "                                                                        n_folds=2)\n",
    "\n",
    "#print(\"Score:\", svc_linear_score)\n",
    "#print(\"Confusion matrix:\", svc_linear_cm, sep='\\n')\n",
    "#print(\"Classification report:\", svc_linear_rep, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVC (sigmoid kernel)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#svc_sigmoid = SVC(kernel='linear')\n",
    "#svc_sigmoid_score, svc_sigmoid_rep, svc_sigmoid_cm, svc_sigmoid_clf = k_fold_model_select(dataset_valid_num, features, label, svc_sigmoid,\n",
    "                                                                        n_folds=2)\n",
    "\n",
    "#print(\"Score:\", svc_sigmoid_score)\n",
    "#print(\"Confusion matrix:\", svc_sigmoid_cm, sep='\\n')\n",
    "#print(\"Classification report:\", svc_sigmoid_rep, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVC (radial basis function kernel)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#svc_rbf = SVC(kernel='rbf')\n",
    "#svc_rbf_score, svc_rbf_rep, svc_rbf_cm, svc_rbf_clf = k_fold_model_select(dataset_valid_num, features, label, svc_rbf,\n",
    "                                                                        n_folds=2)\n",
    "\n",
    "#print(\"Score:\", svc_rbf_score)\n",
    "#print(\"Confusion matrix:\", svc_sigmoid_cm, sep='\\n')\n",
    "#print(\"Classification report:\", svc_sigmoid_rep, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test some ensamble methods:\n",
    "\n",
    "Starting by **AdaBoost**, this algorithm enables to, in each of its iterations, specialize a base classifier for the instances incorrectly classified in the previous iterations. We'll use as base classifier the Decision Tree Classifier, as it is very volatile to the data in which it is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AdaBoost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ab = AdaBoostClassifier(base_classifier=dt)\n",
    "ab_score, ab_rep, ab_cm, ab_clf = k_fold_model_select(dataset_valid_num, features, label, ab)\n",
    "\n",
    "print(\"Score:\", ab_score)\n",
    "print(\"Confusion matrix:\", ab_cm, sep='\\n')\n",
    "print(\"Classification report:\", ab_rep, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
