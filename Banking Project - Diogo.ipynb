{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banking Project\n",
    "\n",
    "***\n",
    "\n",
    ">The bank wants to improve their services. For instance, the bank managers have only vague idea, who is a good client (whom to offer some additional services) and who is a bad client (whom to watch carefully to minimize the bank loses). Fortunately, the bank stores data about their clients, the accounts (transactions within several months), the loans already granted, the credit cards issued. The bank managers hope to improve their understanding of customers and seek specific actions to improve services. A mere application of a discovery tool will not be convincing for them.  \n",
    "\n",
    ">To test a data mining approach to help the bank managers, it was decided to address two problems, a descriptive and a predictive one. While the descriptive problem was left open, the predictive problem is the prediction of whether a loan will end successfuly.\n",
    "\n",
    "> _ - in Banking Case Description, ECAC Moodle Page_\n",
    "\n",
    "***\n",
    "\n",
    "[Kaggle Challenge Page](https://www.kaggle.com/)\n",
    "\n",
    "The steps performed are as follows:\n",
    "* Data Loading and Cleaning\n",
    "* Descriptive Data Mining & Feature Engineering\n",
    "* Predictive Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "For this work we will use the common tools in a data scientist and engineer arsenal. All of them work together in a seamless fashion, as well as with the Jupyter Notebook (this enhanced interactive document).\n",
    "\n",
    "* **Numpy** is the fundamental package for scientific computing with Python\n",
    "* **Pandas** provides high-performance, easy-to-use data structures (_e.g._ data frames) and data analysis tools\n",
    "* **Matplotlib** implements plotting functionality\n",
    "* **Scikit Learn** aggregates advanced machine learning tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Cleaning\n",
    "\n",
    "A key initial step in every data mining work is to clean the data. This reduces the occurence of future unexpected behaviors and gives a preliminary insight over the \"raw\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **transactions** records describe transactions on accounts, representing dynamic characteristics of the accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv('./data/banking - transaction.csv', \n",
    "                              sep=';',\n",
    "                              parse_dates=['date'],\n",
    "                              infer_datetime_format=True,\n",
    "                              dtype={'bank':np.str},\n",
    "                              index_col='trans_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k_symbol** name is not very represent representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactions_df = transactions_df.rename(columns={\n",
    "    'k_symbol': 'trans_char',\n",
    "    'date': 'date created'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accounts** records contain static characteristics of the accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_df = pd.read_excel('./data/banking.xlsx', \n",
    "                            sheetname='account',\n",
    "                            parse_dates=['date'],\n",
    "                            infer_datetime_format=True,\n",
    "                            index_col='account_id'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **clients** records describe static characteristics of the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clients_df = pd.read_excel('./data/banking.xlsx',\n",
    "                           sheetname='client',\n",
    "                           index_col='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clients_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **birth_number** feature is not readable in this representation. We have, then, to parse it and transform it into two new columns: **birthday** and **gender**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clients_df['gender'] = clients_df.apply(lambda c: 'Male' if c['birth_number'] % 10000 < 5000 else 'Female', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def normalize_birth_number(client):\n",
    "    birth_number = int(client['birth_number'])\n",
    "    year = birth_number // 10000\n",
    "    month = (birth_number // 100) % 100\n",
    "    day = birth_number % 100\n",
    "    \n",
    "    month = month if month < 50 else month - 50\n",
    "    \n",
    "    return  \"{0:02d}{1:02d}{2:02d}\".format(year, month, day)\n",
    "\n",
    "\n",
    "clients_df['birth_number'] = clients_df.apply(normalize_birth_number, axis=1) # month - 50 on females\n",
    "clients_df['birthday'] = pd.to_datetime(clients_df['birth_number'], format='%y%m%d')\n",
    "clients_df['birthday'] = clients_df.apply(\n",
    "    lambda c: c['birthday'] if c['birthday'].date() <= date.today() else (c['birthday'] - pd.tseries.offsets.DateOffset(years=100)), \n",
    "    axis=1) # if infered year > 2015 the it is in the 19's\n",
    "clients_df = clients_df.drop('birth_number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clients_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dispositions** records relate a client with an account (being useful in join operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dispositions_df = pd.read_excel('./data/banking.xlsx',\n",
    "                                sheetname='disposition',\n",
    "                                index_col='disp_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispositions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **payment_orders** records, like **transaction** records, represent another dynamic characteristic of accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "payment_orders_df = pd.read_excel('./data/banking.xlsx',\n",
    "                                  sheetname='payment order',\n",
    "                                  index_col='order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "payment_orders_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **loans** records describe information of a loan for an account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans_df = pd.read_excel('./data/banking.xlsx',\n",
    "                         sheetname='loan',\n",
    "                         parse_dates=['date'],\n",
    "                         infer_datetime_format=True,\n",
    "                         index_col='loan_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **credit_cards** records describes static information of a credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_cards_df = pd.read_excel('./data/banking.xlsx',\n",
    "                                sheetname='credit card',\n",
    "                                parse_dates=['issued'],\n",
    "                                infer_datetime_format=True,\n",
    "                                index_col='card_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_cards_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **districts** records provide demographic information about a district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "districts_df = pd.read_excel('./data/banking.xlsx',\n",
    "                             sheetname='district',\n",
    "                             index_col='A1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column labels provided lack any useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "districts_df = districts_df.rename(columns={\n",
    "        'A2': 'district_name',\n",
    "        'A3': 'region',\n",
    "        'A4': 'no_inhabitants',\n",
    "        'A5': 'no_municipalities_w_inhabitants_<499',\n",
    "        'A6': 'no_municipalities_w_inhabitants_500-1999',\n",
    "        'A7': 'no_municipalities_w_inhabitants_2000-9999',\n",
    "        'A8': 'no_municipalities_w_inhabitants_>10000',\n",
    "        'A9': 'no_cities',\n",
    "        'A10': 'ratio_urban_inhabitants',\n",
    "        'A11': 'average_salary',\n",
    "        'A12': 'unemployment_rate_95',\n",
    "        'A13': 'unemployment_rate_96',\n",
    "        'A14': 'no_enterpreneurs_per_1000_inhabitants',\n",
    "        'A15': 'no_commited_crimes_95',\n",
    "        'A16': 'no_commited_crimes_96',\n",
    "    })\n",
    "\n",
    "districts_df.index.name = 'district_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if the types infered by Pandas library are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that **unemployment_rate_95** and **no_commited_crimes_95** are loaded as objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df['unemployment_rate_95'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df['no_commited_crimes_95'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both use a question mark to demark missing values. We'll convert properly those columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df['unemployment_rate_95'] = pd.to_numeric(districts_df['unemployment_rate_95'], errors='coerce')\n",
    "districts_df['no_commited_crimes_95'] = pd.to_numeric(districts_df['no_commited_crimes_95'], errors='coerce')\n",
    "\n",
    "districts_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive Data Mining & Feature Engineering\n",
    "\n",
    "This first section aims at providing ways to better understand and extract value from the data. This is mostly accoplished by gathering descriptive statistics and ploting.\n",
    "\n",
    "Considering this gathered knowledge, the datasets are edited and joined into useful intermediate format, which represent the main entities in the data, and then in a format in which the machine learning algorithms are able to understand (most of the times a single matrix, and most of the times without missing values).\n",
    "\n",
    "The **loans** relate to the remainder entities through the **account** they are linked to. Therefore, the remainder entities should be summarized in such a way that each of the **accounts** information is given in a single row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before making any assumption, we should extract simple statistics about the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**type**, **operation** and **trans_char** seem all to represent the same information. Lets evaluate that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"type:\", transactions_df['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"operation:\", transactions_df['operation'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"trans_char:\", transactions_df['trans_char'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**operation** seems irrelevant give **type**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df_e = transactions_df.drop('operation', axis=1).copy() # 'e' for edited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its also irrelevant the distinction between *withrawal* and *withrawal in cash*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = transactions_df_e['type'] == 'withdrawal in cash'\n",
    "transactions_df_e.ix[mask, 'type'] = ('withdrawal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an additional column to store the signed amount (given by the type of operation), and another with the normalized **signed_amount** value, according to the **balance** previous to the operation (if an operation is the first one, we store 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_df_e['signed_amount'] = transactions_df_e.apply(lambda x: - x['amount'] if x['type'] == 'withdrawal' else x['amount'], axis=1)\n",
    "transactions_df_e['norm_signed_amount'] = transactions_df_e.apply(lambda x: \n",
    "                                                                      0 if (x['balance'] - x['signed_amount']) == 0 \n",
    "                                                                      else x['signed_amount'] / (x['balance'] - x['signed_amount']), \n",
    "                                                                  axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **trans_char** we are able to extract if the user is pensionist, or if the user has been sanctioned for negative balance, among other things. We will create an additional table with that information, with the values weighted by the **amount**, indexed by **account_id**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select the needed rows from transactions_df\n",
    "trans_temp_df = transactions_df_e[['account_id', 'trans_char', 'norm_signed_amount']].copy()\n",
    "\n",
    "# remove the rows where an empty string is present\n",
    "mask = trans_temp_df.trans_char != ' '\n",
    "trans_temp_df = trans_temp_df.ix[mask]\n",
    "\n",
    "# remove the rows containing NaN\n",
    "trans_temp_df = trans_temp_df.dropna(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the dataframe indexed by account_id\n",
    "account_features_df = trans_temp_df[['account_id']].drop_duplicates(subset=['account_id'])\n",
    "account_features_df = account_features_df.set_index('account_id')\n",
    "\n",
    "# create the count columns, corresponding to the data countained in\n",
    "    # pension\n",
    "    # interest credited\n",
    "    # household\n",
    "    # statement\n",
    "    # insurance payment\n",
    "    # sanction for negative balance\n",
    "    # loan payment\n",
    "\n",
    "def create_trans_count_col(df, val):\n",
    "    new_df = df.ix[df['trans_char'] == val].groupby('account_id').sum()\n",
    "    new_df = new_df.rename(columns={'norm_signed_amount':val})\n",
    "    return new_df\n",
    "\n",
    "additional_dfs = [create_trans_count_col(trans_temp_df, val) for val in trans_temp_df['trans_char'].unique()]\n",
    "account_features_df = account_features_df.join(additional_dfs)\n",
    "\n",
    "account_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge it with the **accounts** dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accounts_df = accounts_df.join(account_features_df)\n",
    "accounts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we'll look into the **normalized signed amount** and **type** of each operation.\n",
    "\n",
    "We will extract, for each **account**, the *count*, *mean* and *standard deviation* of operation values, as well as *mean* and *standard deviation* of the number of days between each **operation**, grouped by operation **type**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_df = transactions_df_e[['account_id', 'type', 'norm_signed_amount', 'date']].copy()\n",
    "\n",
    "# create columns with dates converted to days since 01-01-1970\n",
    "#temp_df['date_days'] = temp_df.apply(lambda x: (x['date'] - pd.datetime(1970,1,1)).days, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort firstly by account_id, then by date\n",
    "temp_df = temp_df.sort_values(by=['account_id', 'date'])\n",
    "\n",
    "#obtain, by row, the previous date\n",
    "prev_dates = temp_df.groupby('account_id').apply(lambda x: x['date'].shift().fillna(x.iloc[0]['date'])).reset_index(level=0)\n",
    "delta = temp_df['date'] - prev_dates['date']\n",
    "delta = delta.astype(\"timedelta64[D]\")\n",
    "temp_df['date_delta'] = delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#temp_df = temp_df.drop('date', axis='columns')\n",
    "\n",
    "def summarize_transactions(df):\n",
    "    summaries_dfs = []\n",
    "    for tp in df['type'].unique():\n",
    "        tmp_df = temp_df[temp_df['type'] == tp].drop('type', axis=1)\n",
    "        \n",
    "        tmp_grp_df = tmp_df.groupby('account_id').agg([np.count_nonzero, np.average, np.std])\n",
    "        \n",
    "        ops_df = tmp_grp_df['norm_signed_amount']\n",
    "        ops_df = ops_df.rename(columns={'count_nonzero': tp + '_cnt',\n",
    "                                        'average': tp + '_avg',\n",
    "                                        'std': tp + '_std',\n",
    "                                       })\n",
    "        \n",
    "        dates_df = tmp_grp_df['date_delta']\n",
    "        dates_df = dates_df.drop('count_nonzero', axis='columns')\n",
    "        dates_df = dates_df.rename(columns={'average': tp + '_dates_avg',\n",
    "                                            'std': tp + '_dates_std',\n",
    "                                           })\n",
    "        joined_summary_df = ops_df.join(dates_df)\n",
    "        summaries_dfs.append(joined_summary_df)\n",
    "    \n",
    "    # now concatenate the summaries_dfs\n",
    "    summaries_df = pd.concat(summaries_dfs, axis='columns')\n",
    "    return summaries_df\n",
    "\n",
    "trans_summaries_df = summarize_transactions(temp_df)\n",
    "trans_summaries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge with **accounts** dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_df = accounts_df.join(trans_summaries_df)\n",
    "accounts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clients\n",
    "\n",
    "Now we'll take a look at the features that characterize each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clients_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for each user corresponds a district, we'll look into the corresponding table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider both the individual districts or the regions. \n",
    "In order to evaluate if the generalization for regions is reduces or not some interesting events, we'll use PCA to reduce the dimensionality of the table to two dimensions, therefore making it possible to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df_e = districts_df.drop(['district_name', 'region'], axis='columns')\n",
    "districts_df_e = districts_df_e.fillna(0)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "districts_reduced = pca.fit_transform(districts_df_e.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts_df_e['a'] = districts_reduced[:, 0]\n",
    "districts_df_e['b'] = districts_reduced[:, 1]\n",
    "\n",
    "# convert regions to integers, to use them as colors\n",
    "regions = districts_df.apply(lambda x: x['region'][1], axis='columns').astype(int)\n",
    "\n",
    "districts_df_e.plot(kind='scatter', x='a', y='b', c=regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we conclude that probably it is beneficial to work with districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clients_districts_df = clients_df.join(districts_df.drop(['region', 'district_name'], axis='columns'))\n",
    "\n",
    "# we no longer need district_id column\n",
    "clients_districts_df = clients_districts_df.drop('district_id', axis='columns')\n",
    "\n",
    "clients_districts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll merge **Credit Cards** and **Clients** information with the accounts.\n",
    "\n",
    "As there may be multiple credit cards and clients associated with a given account, we have to devise a certain heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispositions_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispositions_df.type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll simply use the information about the owner of the account.\n",
    "\n",
    "We'll check the data if there is really only one owner per account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispositions_owners_df = dispositions_df[dispositions_df['type'] == 'OWNER'].drop('type', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "owners_per_account_df = dispositions_owners_df.groupby(['account_id']).count()\n",
    "\n",
    "owners_per_account_df[owners_per_account_df['client_id'] != 1].count(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge **clients** with **accounts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_clients_df = accounts_df.join(dispositions_owners_df).join(clients_districts_df) \\\n",
    "                      .drop(['account_id', 'client_id'], axis='columns')\n",
    "\n",
    "accounts_clients_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loans\n",
    "\n",
    "We want to keep a summary of the history of loans related to a given account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(loans_df.status.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference:\n",
    " * **A-** contract finished, no problems\n",
    " * **B-** contract finished, loan not payed\n",
    " * **C-** running contract, ok so far\n",
    " * **D-** running contract, client in debt\n",
    " \n",
    "Lets  see how the loans distribute accross the four categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans_df.groupby('status').count()['account_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also a lot of contracts running, but most importantly there is a gret disparity between the two counts of the different loans results. We must be aware of this when training the algorithms. As we want to identify properly the loans that will have the **B** status, we have to be carefull when structuring the training and testing datasets.\n",
    "\n",
    "In order to summarize this data, we want to obtain the *count* of loans in each state, as well as the *average* and *standard deviation* of the amount and duration associated with each loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summarize_loans(df):\n",
    "    summaries_dfs = []\n",
    "    for tp in df['status'].unique():\n",
    "        tmp_df = df[df['status'] == tp].drop('status', axis=1)\n",
    "        \n",
    "        tmp_grp_df = tmp_df.groupby('account_id').agg([np.count_nonzero, np.average, np.std])\n",
    "        \n",
    "        amount_df = tmp_grp_df['amount']\n",
    "        amount_df = amount_df.rename(columns={'count_nonzero': tp + '_cnt',\n",
    "                                           'average': tp + '_amount_avg',\n",
    "                                           'std': tp + '_amount_std',\n",
    "                                          })\n",
    "        \n",
    "        duration_df = tmp_grp_df['duration']\n",
    "        duration_df = duration_df.drop('count_nonzero', axis='columns')\n",
    "        duration_df = duration_df.rename(columns={'average': tp + '_duration_avg',\n",
    "                                                  'std': tp + '_duration_std',\n",
    "                                                 })\n",
    "        joined_summary_df = amount_df.join(duration_df)\n",
    "        summaries_dfs.append(joined_summary_df)\n",
    "    \n",
    "    # now concatenate the summaries_dfs\n",
    "    summaries_df = pd.concat(summaries_dfs, axis='columns')\n",
    "    return summaries_df\n",
    "\n",
    "\n",
    "loans_summary_df = summarize_loans(loans_df)\n",
    "loans_summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now merge with accounts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_loans_df = accounts_clients_df.join(loans_summary_df)\n",
    "accounts_loans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_loans_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've gathered 55 features to characterize each account, gathering information about the accounts, users (and respective credit cards), transactions and history of loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictive Data Mining\n",
    "\n",
    "The above-mentioned accounts-indexed dataframe, combined with the specification of each loan (duration and amount), constitutes all the needed information to train our algorithms.\n",
    "\n",
    "\n",
    "Firstly we have to prepare a dataframe such that the machine-learning algorithms are able to undertand it. In order to accomplish that, we will merge this **accounts_loans_df** dataframe with the original **loans_df** dataframe. For new loans that arive, the process will be the same, therefore we want to encapsulate the process in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accounts_final_df = accounts_loans_df\n",
    "\n",
    "def join_loans_accounts(l_df):\n",
    "    return l_df.join(accounts_final_df, lsuffix='new_loan_', rsuffix='account_')\n",
    "\n",
    "loans_df.join(accounts_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
